from tenacity import retry, stop_after_attempt, wait_fixed
from core.dependencies import (
    get_openai_client,
    get_langchain_chain,
)
from langchain_core.messages import HumanMessage, SystemMessage
from typing import List, Optional
from openai import BadRequestError
from utils.log_utils import get_logger
import json

logger = get_logger(__name__)

# 1. ì¼ë°˜ OpenAI ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
@retry(stop=stop_after_attempt(3), wait=wait_fixed(1))
async def call_openai_stream_async(
    history: List[dict],
    functions: Optional[list] = None,
    function_call: str = "auto"
):
    client = await get_openai_client()
    params = {
        "model": "gpt-4o",
        "messages": history,
        "stream": True
    }
    if functions:
        params["functions"] = functions
        params["function_call"] = function_call

    return await client.chat.completions.create(**params)

# 2. ì¼ë°˜ OpenAI ì™„ë£Œí˜• í˜¸ì¶œ
@retry(stop=stop_after_attempt(3), wait=wait_fixed(1))
async def call_openai_completion(history: list, functions=None, function_call="auto"):
    client = await get_openai_client()
    params = {
        "model": "gpt-4o",
        "messages": history,
        "stream": False
    }
    if functions:
        params["functions"] = functions
        params["function_call"] = function_call
    response = await client.chat.completions.create(**params)
    return response.choices[0].message.content, response

# 3. Langchain ê¸°ë°˜ LLM í˜¸ì¶œ (ë¹„ë™ê¸°)
async def call_langchain_chat(prompt: str, system_prompt: str = None) -> str:
    chain = get_langchain_chain()

    messages = []
    if system_prompt:
        messages.append(SystemMessage(content=system_prompt))
    messages.append(HumanMessage(content=prompt))

    response = await chain.ainvoke(messages)
    return response.content

@retry(stop=stop_after_attempt(3), wait=wait_fixed(1))
async def openai_completion_with_function_call(
    history,
    functions,
    function_map,
    bot=None,
    max_func_calls=5
):
    client = await get_openai_client()
    params = {
        "model": "gpt-4o", # ì‹¤ì‹œê°„ ì‘ë‹µ ì†ë„ë¥¼ ìœ„í•´ mini ì‚¬ìš©
        "messages": filter_for_openai(history),
        "stream": False,
        "functions": functions,
        "function_call": "auto"
    }

    call_count = 0
    while call_count < max_func_calls:
        print(f"[openai_completion_with_function_call] call_count={call_count} | params={params}")
        try:
            response = await client.chat.completions.create(**params)
        except BadRequestError as e:
            print(e)
        
        if not response.choices:
            raise ValueError("GPT ì‘ë‹µì— choicesê°€ ì—†ìŠµë‹ˆë‹¤.")
        msg = getattr(response.choices[0], "message", None)
        if msg is None:
            raise AttributeError("choices[0]ì— messageê°€ ì—†ìŠµë‹ˆë‹¤.")

        # 1. function_call ì—¬ë¶€ íŒë‹¨
        function_call = getattr(msg, "function_call", None)
        if function_call:
            call_count += 1
            func_name = getattr(function_call, "name", None)
            arguments = getattr(function_call, "arguments", None)
            if func_name is None or arguments is None:
                raise ValueError("function_call object is missing 'name' or 'arguments'")

            if not arguments or arguments.strip() == "":
                args = {}
            else:
                try:
                    args = json.loads(arguments)
                except Exception as e:
                    logger.warning(f"[WARN] arguments json decode error: {arguments} | {e}")
                    args = {}

            if "query" not in args:
                logger.warning(f"[function_call] 'query' ì¸ìê°€ ì—†ìŒ â†’ ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ëŒ€ì²´")
                args["query"] = history[-1]["content"]

            # ì‹¤ì œ function ì‹¤í–‰
            result = await function_map[func_name](**args)
            # historyì— function ê²°ê³¼ append
            if bot is not None:
                function_msg_id = bot.save_to_db(bot.user_id, "function", json.dumps({"name": func_name, "result": result}, ensure_ascii=False))
            else:
                function_msg_id = history[-1]["id"] + 1
            
            history.append({
                "role": "function",
                "name": func_name,
                "content": json.dumps(result, ensure_ascii=False) if not isinstance(result, str) else result,
                "id":function_msg_id
            })

            if bot is not None:
                bot.save_history(history)
            # function-call í›„ ë£¨í”„ ì¬ì‹œì‘
            params["messages"] = filter_for_openai(history)
            continue  # ë‹¤ì‹œ ë°˜ë³µë¬¸ ì§„ì…

        # 2. function_callì´ ì•„ë‹ˆë¼ë©´ assistant ë‹µë³€ ë°˜í™˜
        if hasattr(msg, "content"):
            return msg.content  # (í•„ìš”ì‹œ, return msg.content, response)
        else:
            # contentê°€ ì—†ì„ ë•Œ(ì´ë¡ ìƒ ê±°ì˜ ì—†ìŒ)
            return ""

    # í•¨ìˆ˜ í˜¸ì¶œ ìµœëŒ€ì¹˜ ì´ˆê³¼
    raise RuntimeError("Function-call ë°˜ë³µ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")

# services/openai_client.py
from typing import AsyncGenerator
from openai.types.chat import ChatCompletionChunk

@retry(stop=stop_after_attempt(3), wait=wait_fixed(1))
async def openai_stream_with_function_call(
    history: list,
    functions: list,
    function_map: dict,
    bot=None,
    max_func_calls=3
) -> AsyncGenerator[str, None]:
    """
    function_call + streaming ê°€ëŠ¥í•œ GPT ì‘ë‹µ ìƒì„±ê¸°
    """
    client = await get_openai_client()
    current_history = filter_for_openai(history)

    call_count = 0
    while call_count < max_func_calls:
        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=current_history,
            stream=True,
            functions=functions,
            function_call="auto"
        )

        collected = ""
        func_call_detected = False
        arguments_collected = ""
        function_name = None
        async for chunk in response:
            delta = chunk.choices[0].delta
            finish_reason = chunk.choices[0].finish_reason

            # âœ… function_call ê°ì§€
            if delta.function_call:
                func_call_detected = True
                if delta.function_call.name:
                    function_name = delta.function_call.name
                if delta.function_call.arguments:
                    arguments_collected += delta.function_call.arguments
                continue  # ì—¬ê¸°ì„œ break í•˜ë©´ ì•ˆ ë¨ â—

            if delta.content:
                collected += delta.content
                yield delta.content  # ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜

        if func_call_detected:
            call_count += 1
            try:
                args = json.loads(arguments_collected) if arguments_collected.strip() else {}
                logger.info(f"[function_call] arguments raw: {arguments_collected}")
            except Exception as e:
                logger.warning(f"[WARN] function_call arguments íŒŒì‹± ì‹¤íŒ¨: {arguments_collected} | {e}")
                args = {}

            if "query" not in args:
                logger.warning(f"[function_call] 'query' ì¸ìê°€ ì—†ìŒ â†’ ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ëŒ€ì²´")
                args["query"] = history[-1]["content"]
            # ğŸ”§ function ì‹¤í–‰
            result = await function_map[function_name](**args)
            function_msg_id = bot.save_to_db(bot.user_id, "function", json.dumps(result, ensure_ascii=False)) if bot else len(history)

            history.append({
                "role": "function",
                "name": function_name,
                "content": json.dumps(result, ensure_ascii=False),
                "id": function_msg_id
            })
            current_history = filter_for_openai(history)
            if bot:
                bot.save_history(history)
            continue  # GPT ì¬í˜¸ì¶œ

        # function_call ì—†ì´ ì •ìƒ ì¢…ë£Œ â†’ ì €ì¥
        if bot:
            assistant_msg_id = bot.save_to_db(bot.user_id, "assistant", collected)
            history.append({"role": "assistant", "content": collected, "id": assistant_msg_id})
            bot.save_history(history)

        break  # ì¢…ë£Œ

    if call_count >= max_func_calls:
        raise RuntimeError("Function-call ë°˜ë³µ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")


@retry(stop=stop_after_attempt(5), wait=wait_fixed(1))
async def get_openai_embedding(text: str):
    # OpenAI text-embedding-3-small ì¶”ì²œ (ì§§ì€ í…ìŠ¤íŠ¸)
    client = await get_openai_client()
    resp = await client.embeddings.create(
        input=text,
        model="text-embedding-3-small"
    )
    return resp.data[0].embedding

def filter_for_openai(history: list) -> list:
    """
    OpenAI APIì— ì „ë‹¬í•  ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (id ë“± ë¶ˆí•„ìš” í•„ë“œ ì œê±°)
    """
    # OpenAI API docs: role, content, name, function_call ë“±ë§Œ í—ˆìš©
    allowed_keys = {"role", "content", "name", "function_call"}
    openai_inputs = [
        {k: v for k, v in msg.items() if k in allowed_keys}
        for msg in history
    ]
    for msg in openai_inputs:
        if msg["role"] == "function" and "name" not in msg:
            content = json.loads(msg['content'])
            msg['name'] = content['name']
            msg['content'] = content['result']
        if msg['role'] == "summary":
            msg['role'] = "function"
            msg["name"] = "chat_summarizer"
        assert msg.get("role") in ("system", "user", "assistant", "function"), msg
        assert isinstance(msg.get("content", ""), str), msg
    return openai_inputs